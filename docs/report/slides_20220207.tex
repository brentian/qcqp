\documentclass[aspectratio=1610, 9pt]{beamer}
\usepackage[english]{babel}
\usepackage{amsmath, subfiles, bm, hyperref, graphicx}
\usepackage{mathrsfs}
\usepackage{caption, longtable, booktabs}
\usepackage{cancel}

% my cmd
%%%%%%%%%%%%%%%%
% start my commands
%%%%%%%%%%%%%%%%
\newcommand{\lm}{\lambda_\textbf{max}}
\newcommand{\trace}{\textbf{trace}}
\newcommand{\diag}{\textbf{diag}}
\newcommand{\rank}{\textbf{rank}}
\newcommand{\model}[1]{(\textbf{#1})}
\newcommand{\mx}{\mathbf{\max}\;}
\newcommand{\mn}{\mathbf{\min}\;}
\newcommand{\st}{\mathrm{s.t.\;}}
\newcommand{\ex}{\mathbf E}
\newcommand{\dx}{\;\bm dx}
\newcommand{\pr}{\mathbf P}
\newcommand{\id}{\mathbf I}
\newcommand{\bp}{\mathbb P}
\newcommand{\be}{\mathbb E}
\newcommand{\bi}{\mathbb I}
\newcommand{\bxi}{{\bm \xi}}
\newcommand{\va}{\mathbf{Var}}
\newcommand{\dif}{\mathbf{d}}
\newcommand{\minp}[2]{\min\{#1, #2\}}
\newcommand{\intp}{\mathbf{int}}
\newcommand{\apex}{\mathbf{apex}}
\newcommand{\conv}{\mathbf{conv}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\redsf}[1]{\textcolor{red}{\textsf{#1}}}
\newcommand{\real}{\mathbb{R}}
\usefonttheme[onlymath]{serif}

\setbeamertemplate{theorems}[numbered]
%%%%%%%%%%%%%%%%
% finish my commands
%%%%%%%%%%%%%%%%
% set the toc to be using the number
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]
\setbeamertemplate{caption}[numbered]

\makeatletter
\patchcmd{\beamer@sectionintoc}
{\ifnum\beamer@tempcount>0}
{\ifnum\beamer@tempcount>-1}
{}
{}
\beamer@tocsectionnumber=0
\makeatother


\hypersetup{pdfstartview={Fit}}

\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[        currentsection,         currentsubsection,         subsectionstyle=show/shaded/hide,]
    \end{frame}
}


\defbeamertemplate{section page}{mine}[1][]{%
    \begin{centering}
        \vskip1em\par
        \begin{beamercolorbox}[sep=12pt, center]{part title}
            \usebeamerfont{section title}\insertsection\par
        \end{beamercolorbox}
    \end{centering}
}

\defbeamertemplate{subsection page}{mine}[1][]
{
    \begingroup
    \begin{beamercolorbox}[sep=8pt, center]{subsection title}
        \usebeamerfont{subsection title}\insertsubsection\par
    \end{beamercolorbox}
    \endgroup
}


\setbeamertemplate{section page}[mine]
\setbeamertemplate{subsection page}[mine]
% default navigation
\defbeamertemplate{navigation symbols}{mydefault}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
% standout navigation
\defbeamertemplate{navigation symbols}{standout}{%
  \setbeamertemplate{navigation symbols}{}%
}

\BeforeBeginEnvironment{frame}{%
  \setbeamertemplate{navigation symbols}[mydefault]%
}

\makeatletter
\define@key{beamerframe}{standout}[true]{%
  \setbeamertemplate{navigation symbols}[standout]
}
\makeatother


\begin{document}


\title{QCQP: Progress Report}

\author{
  Chuwen Zhang
}
\maketitle


\begin{frame}{The QCQP}
  We consider the QCQP,
  \begin{equation}
    \label{eq:inhoqcqp}
    \begin{aligned}
      \model{QCQP} \quad \mx \quad & x^T Qx + q^T x                                        \\
      \textrm{s.t.} \quad          & x^T A_i x + a_i^T x \hspace{0.27em} (\le, =, \ge) b_i \\
    \end{aligned}
  \end{equation}
\end{frame}
\section{Improving the MSC relaxation}
\begin{frame}[allowframebreaks]{Rank-\(r\) Indefiniteness}

  Formally, a quadratic inequality induced by a symmetric matrix \(A\) can be expressed as the following,

  \begin{equation}
    x^T \left(\sum_{j\in J_+} \lambda_j v_jv_j^T\right)x +a^Tx \le b + x^T\left(\sum_{j\in J_-} \lambda_j v_jv_j^T\right)x
  \end{equation}

  \begin{itemize}
    \item For clarity, we say a quadratic inequality \(x^TAx + a^Tx \le b\) is \emph{rank-\(r\) indefinite} if first \(r\) eigenvalues are negative.
    \item In comparison, a maximization problem with matrix \(Q\) is rank-\(r\) indefinite if last \(r\) eigenvalues are nonnegative.
  \end{itemize}

  \framebreak

  Consider quadratic maximization with no other constraint,
  \begin{equation}
    \begin{aligned}
      \mx \quad & x^TQx + q^Tx \\
    \end{aligned}
  \end{equation}

  except for a regularity on \(x\) to avoid unboundedness.
  \begin{itemize}
    \item ball: \(x\in B(0, \delta)\),
    \item box: \(x\in [0, 1]^n\)
    \item ellipsoid: \(x \in \{x: x^TAx + a^Tx \le \delta^2\}\)
  \end{itemize}

  \begin{equation}\label{eq:qp_unc_rr_conic}
    \begin{aligned}
      \mx \quad & \sum_{j\in J_+} \lambda_j y_j - x^TQ_-x  + q^Tx \\
      \st \quad & y_j \ge (x_j^T v_j)^2, j= 1, ..., \red{r}
    \end{aligned}
  \end{equation}

  In the ball-constrained case, one can bound \(y\) by,
  \begin{equation}
    y^Te \le \|x\|^2 \le \delta^2
  \end{equation}
\end{frame}

\begin{frame}[allowframebreaks]{Revised MSC Relaxation}
  Suppose now we have QCQP with \(m\) constraints, where
  \begin{align*}
    Q = V_0\Lambda_0V_0^T, A_i = V_i\Lambda_iV_i^T
  \end{align*}
  The old MSC formulation is the following,
  \begin{align}
    \nonumber \mathrm{Maximize}\quad & y_0 ^T\diag(\Lambda_0) + q^Tx                         \\
    \mathrm{s.t.} \quad              & V_i^T x = z_i                             & i=0,...,m \\
                                     & y_i ^T\diag(\Lambda_i)  + a_i^Tx  \le b_i & i=1,...,m \\
    \label{quad}                     & y_i = z_i \circ z_i                       & i=0,...,m
  \end{align}
  \begin{itemize}
    \item In this formulation, we need \((m + 1) \times n\) auxillary pairs \(\{(z_i, y_i)\}_{i=0}^m\) by allowing different bases \(\{V_i\}_{i=0}^m\)
    \item Actually this may not be necessary...
  \end{itemize}

  \framebreak
  Consider one indefinite \(A\) with the induced inequality,

  \[\mathcal A =  \{x \mid x^TAx +a^Tx \le b\}\]

  Now consider conditions/reformulations to make \(\mathcal A\) convex.

  Use a readily available orthonormal vectors \(V \in \real^{n\times\red{r}}\)
  \[\mathcal A =  \{x \mid x^T\left(A + V\red{\Gamma}V^T\right )x +a^Tx \le b + x^T(V\red{\Gamma}V^T)x\}\]
  If \(\Gamma\) is diagonal, we have,
  \[\tilde {\mathcal  A} =  \{x \mid x^T\left(A + V\red{\Gamma}V^T\right )x + a^Tx \le b +\diag(\Gamma)^T y\}\]


  \(\tilde {\mathcal  A}\) is convex iff. \(A + V\red{\Gamma}V^T \succeq 0\). \red{What is the condition for \(V\) to make this possible?} Desired property,
  \begin{enumerate}
    \item \(V\) is low-rank
    \item \(V\) is orthonormal (not necessary)
  \end{enumerate}

\end{frame}
\begin{frame}[allowframebreaks]{Find \(V\) with lowest rank: from \(Q\) or \(\{A_j\}\)}

  We first consider using eigenvectors (e.g. of \(Q\)). Let \(v_1, ..., v_r, r \le n\) be columns of \(V\),

  \begin{itemize}
    \item if \(V\) is nonsingular (\(r = n\)), we use \(n\) basic vectors, it is always possible to find such \(\Gamma\). For example, let \(\sigma = - \lambda_{\min} (A)\), then
          \[A + \sigma \cdot VV^T = A + \sigma I \succeq 0\]
          This is a weaker construction than SD, \cite{jiang_simultaneous_2016}. For example,
          \begin{align*}
            Q = \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}, A = \begin{bmatrix}1 & 0 \\ 0 & -1\end{bmatrix}
          \end{align*}
          \{Q, A\} are not SD but yet \(A + I \succeq 0\)

    \item This means we need \red{at most} \(n\) \((z, y)\) pairs (a straight forward choice).\\
          \red{Can we do even better? find \(r \le n\)}
          \framebreak
    \item if \(V\) is rank-1, \(\Gamma\) may not exist. For example, if \(V = v_1v_1^T\). We know that the smallest eigenvalue of \(A + \alpha v_1v_1^T\) is bounded by the second smallest eigenvalue of \(A\). (Interlacing property)


    \item if \(V\) is singular (\(r < n\)), and \(A\) is negative definite of rank \(m\), for example, \(m \le r\), still may not be possible, for example,
          \[Q = \begin{bmatrix}I_r & \\ & 0_{n-r} \end{bmatrix}, A = \begin{bmatrix}0_r & \\ & - I_{n-r} \end{bmatrix}\]
  \end{itemize}

  \framebreak

  The above question can be written as the following problem:

  For matrices \(\{Q, A_i\}\), find a mutual \(V\) with columns \(v_1, \dots, v_r\) and diagonal matrices \(\{\Lambda_j\}_{j=0}^m\) allow the \red{simultaneous convexification} such that,
  \begin{equation}\label{eq.simul.conv}
    \begin{aligned}
      \mn & \rank\left(V\right)         \\
      - Q & +  V\Lambda_0 V^T \succeq 0 \\
      A_1 & +  V\Lambda_1 V^T \succeq 0 \\
      ...                               \\
      A_j & +  V\Lambda_j V^T \succeq 0 \\
    \end{aligned}
  \end{equation}

  Then do the following,
  \begin{align*}
    z = V^T y , V\in \real^{n \times \red{r}}
  \end{align*}

  \begin{itemize}
    \item In the worst case, \(r=n\) columns (consider using a random normal basis).
    \item Some ideas, S-lemma, \cite{burer_nonlinear_2003}, solve \(X\succeq 0 \Leftrightarrow X= RR^T \)
  \end{itemize}

  \framebreak
  The fact below produces a \redsf{feasible} solution of \eqref{eq.simul.conv}
  \begin{theorem}
    \label{pp.simul.conv.cond1}
    Allow rank-1 decomposition, \(Q = R_0 R_0^T - \hat{Q} , A_j = \hat{A_j}- R_j R_j^T, \red{R_j\in \real^{n\times r_j}}\), assume,

    \[\tilde R = \left [R_0 \mid R_1 \mid ... \mid R_m \right ]   \in \real^{n \times (\sum_j r_j)}\]

    There exists orthonormal matrix \(V \in \real^{n\times r}\) to solve \eqref{eq.simul.conv}, furthermore,

    \begin{enumerate}
      \item \(r= \rank (\tilde{R})\)
      \item There exists \(P_j \in \real^{r\times r_j}\) such that,
            \[ R_j = V P_j\]
    \end{enumerate}
  \end{theorem}

  \begin{proof}
    Compute QR factorization for \(\tilde R \doteq V\cdot R\), truncate orthogonal matrix \(V\) to its rank \(r\). Then \(V\) is the desired bases.
    A feasible choice could be \eqref{eq.simul.conv} \(\Lambda_j = \lambda_j I\) according to the eigenvalues.
  \end{proof}

\end{frame}


\section{The ALM Using Revised MSC}
\begin{frame}[allowframebreaks]{Biaffine Approximation}
  \begin{align}
    \model{MSC} \quad \mx \quad & \diag(\Lambda_0)^Ty + x^T\left(Q - \diag(\Lambda_0)\right)x \\
    \mathrm{s.t.} \quad         & (y, x) \in \Omega                                           \\
                                & (v_i^Tx)^2 \le y_i
  \end{align}
  With \(y\) being an overestimate of \(V^Tx \circ V^Tx\), consider the following system,

  \begin{align*}
    ( \mu_i)    \quad & \xi_i \cdot (v_i^Tx) = y_i           \\
                      & \xi_i^2 \le y_i,  (v_i^Tx)^2 \le y_i
  \end{align*}


  The AL,

  \begin{equation}
    \mathcal L = ... + \sum_{i=1}^r \mu_i \left (\xi_i \cdot(v_i^Tx) - y_i\right) + \frac{\rho}{2}\sum_{i=1}^r \left(\xi_i \cdot (v_i^Tx) - y_i\right)^2
  \end{equation}

  At iteration \(k\),

  \begin{subequations}
    \begin{align}
      \label{eq.admm.iterx}   (x^{k+1}, y^{k+1}) & = \arg\min_{x, y} L\left(x,y,\xi^k,\mu^k\right)                         \\
      \label{eq.admm.iterxi}   \xi^{k+1}         & = \arg\min_{\xi} L\left(x^{k+1}, y^{k+1}, \mu^k\right)                  \\
      \mu^{k+1}                                  & = \mu^k + \tau^k\rho\left((\xi^{k+1})\circ V^T x^{k+1} - y^{k+1}\right)
    \end{align}
  \end{subequations}
  The optimization problem \eqref{eq.admm.iterx} is convex that can be solved effectively. The iteration for \(\xi\) can be decomposed into \(r\) subproblems, at iteration \(k+1\), for \(i = 1, ..., r\),

  \begin{equation}
    \begin{aligned}
      \mn & \mu_i^k \left (\xi_i \cdot(v_i^Tx^{k+1}) - y_i^{k+1}\right) + \frac{\rho}{2} \left(\xi_i \cdot (v_i^Tx^{k+1}) - y_i^{k+1}\right)^2 \\
      \st & \xi_i^2 \le y_i^{k+1}
    \end{aligned}
  \end{equation}

  which is a one dimension problem that allows analytic solution.

  \begin{theorem}
    The termination criterion for ADMM,
    \begin{enumerate}
      \item Primal residual. \(\left \|(V^Tx^{k+1}) \circ \xi^{k+1} - y^{k+1} \right\| \le \epsilon\)
      \item Dual residual. \(\left\|(V^Tx^{k+1}) \circ (\xi^{k+1}-\xi^k)\right\| \le \epsilon\)
    \end{enumerate}
  \end{theorem}
  As a comparison, see the \hyperlink{oldad}{ADMM in original \(x\)}.
\end{frame}

\begin{frame}[allowframebreaks]{ADMM in BB}
  Embed ADMM into BB as a primal method.

  \begin{enumerate}
    \item ADMM is faster then the old one
    \item The strategy is to warm-start ADMM in the subregions. [very fast, from \~90 to \~30]
  \end{enumerate}

\end{frame}


\section{Computational Results}
\begin{frame}
  \begin{enumerate}
    \item We use eigenvectors \(V = V_0\) of \(Q\) to convexify constraints.
    \item We simply choose \(\Lambda_j = \lambda_j I\)
    \item We test on Box QP, QCQP, and QCL.
  \end{enumerate}
\end{frame}
\setlength\pdfpagewidth{16.00cm}%
\setlength\pdfpageheight{20.00cm}%
\begin{frame}[standout]{Box QP}
  Recently, \cite{luo_new_2019} considers rank-\(r\) indefinite QP under linear and convex quadratic constraints. We mark \textsf{Luo, ADMBB} as the method of using ADMM + BB.  Their method is basically the following:
  \begin{itemize}
    \item The ADMM penalizes \(V^Tx = Z\).
    \item The BB procedure is the same, except for the branching rules. In \textsf{Luo, ADMBB}, the pivot is calculated by \(z := \frac{1}{2}(l_z + u_z)\) instead of \(z^*\)
    \item In the case of Box QP, the convex relaxation is the same. They use CPLEX to solve QCP directly.
    \item The instances are generated following the tradition in \cite{le_an_solving_1997}.
  \end{itemize}


  \small
  \begin{table}[h!]
    \centering
    \begin{tabular}{llllll}
      \toprule
      \(n\) & \(r\) & solve\_time & nodes & \#ADMM & method              \\
      \midrule
      50    & 5     & 29.089      & 1122  & -      & grb                 \\
      50    & 5     & 11.059      & 529   & 2      & bb\_msc             \\
      50    & 5     & 8.114       & 245   & 3      & \textsf{Luo, ADMBB} \\
      100   & 5     & 90.549      & 439   & -      & grb                 \\
      100   & 5     & 9.683       & 529   & 4      & bb\_msc             \\
      100   & 5     & 8.776       & 305   & 5      & \textsf{Luo, ADMBB} \\
      150   & 5     & 241.730     & 439   & -      & grb                 \\
      150   & 5     & 31.602      & 537   & 4      & bb\_msc             \\
      150   & 5     & 12.56       & 305   & 8      & \textsf{Luo, ADMBB} \\
      200   & 5     & 0.082       & 3     & -      & grb                 \\
      200   & 5     & 34.344      & 315   & 7      & bb\_msc             \\
      200   & 5     & 58.51       & 187   & 4      & \textsf{Luo, ADMBB} \\
      \bottomrule
    \end{tabular}
    \caption{The box QP instances generated using the method in \cite{luo_new_2019}.
      We terminate Gurobi very early here (gap \(\le e^{-3}\))}


    \begin{itemize}
      \item In Box-QP, our relaxation will produce the feasible solutions, and thus the ADMM or any other primal method is not critical
      \item In \textsf{Luo, ADMBB}, we can also see the number of required or invoked ADMM iterations are limited.
    \end{itemize}
  \end{table}
  \normalsize

\end{frame}

\begin{frame}[standout]{QCQP}

  Below are results for QCQP\(^\dagger\)

  \small
  \begin{table}[h!]
    \centering
    \begin{tabular}{llrrrrrll}
      \toprule
      {} & prob\_num & solve\_time & best\_bound & best\_obj          & node\_time & nodes   & primal\# & method  \\
      \midrule
      0  & 40:10:0   & 500.015     & 4.0959      & 1.8862             & 0.000      & 19963.0 &          & grb     \\
      1  & 40:10:0   & 97.251      & 1.8823      & 1.8818             & 0.054      & 1249.0  & 18       & bb\_msc \\
      0  & 40:20:0   & 500.018     & 3.4412      & 1.0707             & 0.000      & 20530.0 &          & grb     \\
      1  & 40:20:0   & 176.183     & 1.0598      & 1.0591             & 0.027      & 1507.0  & 20       & bb\_msc \\
      0  & 100:10:0  & 1500.040    & 10.7053     & 0.1937             & 0.000      & 6178.0  & -        & grb     \\
      1  & 100:10:0  & 1020.678    & 0.1941      & 0.1933             & 0.121      & 3415.0  & -        & bb\_msc \\
      0  & 100:20:0  & 1500.030    & 11.0759     & 0.2413             & 0.000      & 6160.0  & -        & grb     \\
      1  & 100:20:0  & 1500.551    & 0.2503      & 0.2464             & 0.134      & 2652.0  & -        & bb\_msc \\
      0  & 100:30:0  & 1500.008    & 10.8644     & 0.1360             & 0.00       & 12232.0 & -        & grb     \\
      1  & 100:30:0  & 1008.489    & 0.1358      & 0.1358\(^\dagger\) & 0.22       & 1921.0  & -        & bb\_msc \\
      \bottomrule
    \end{tabular}
    \caption{The randomly generated QCQP (\redsf{rand-qcqp}) instances with \(r = 5\), terminate at \(\epsilon = e^{-5}\) or maximum running time \(1,500\)s. \(\dagger\){still have some precion bugs in branch and cut.}}
  \end{table}
  \begin{table}[h!]
    \begin{tabular}{llrrrrrl}
      \toprule
      {} & prob\_num & solve\_time & best\_bound & best\_obj & node\_time & nodes   & method  \\
      \midrule
      0  & 40:10:0   & 1500.027    & 5.2998      & 4.3267    & 0.000      & 63147.0 & grb     \\
      1  & 40:10:0   & 1500.321    & 5.4472      & 4.1006    & 0.034      & 15502.0 & bb\_msc \\
      0  & 40:20:0   & 1500.010    & 4.8955      & 3.4024    & 0.000      & 60377.0 & grb     \\
      1  & 40:20:0   & 1500.814    & 5.1576      & 3.5756    & 0.041      & 11651.0 & bb\_msc \\

      \bottomrule
    \end{tabular}
    \caption{The randomly generated QCQP (\redsf{rand-qcqp}) instances with \(r = 10\), terminate at \(\epsilon = e^{-5}\) or maximum running time \(1,500\)s. }
  \end{table}
  \begin{table}[h!]
    \begin{tabular}{llrrrrrl}
      \toprule
      {} & prob\_num   & solve\_time & best\_bound & best\_obj & node\_time & nodes   & method  \\
      \midrule
      0  & QPLIB\_0911 & 1099.673    & 32.1476     & 32.1476   & 0.000      & 1123.0  & grb     \\
      1  & QPLIB\_0911 & 1515.119    & 35.6267     & 29.3279   & 0.090      & 2702    & bb\_msc \\
      2  & QPLIB\_1055 & 70.978      & 33.0370     & 33.0370   & 0.000      & 453.0   & grb     \\
      3  & QPLIB\_1055 & 1505.870    & 35.1088     & 32.1447   & 0.022      & 9703    & bb\_msc \\
      4  & QPLIB\_1940 & 1500.002    & 42.5500     & 37.5080   & 0.000      & 67103.0 & grb     \\
      5  & QPLIB\_1940 & 1500.390    & 43.5463     & 29.4716   & 0.208      & 1434    & bb\_msc \\
      \bottomrule
    \end{tabular}
    \caption{The QPLIB QCQP (\redsf{qplib-qcqp}) instances. Note this subset of QPLIB instances are provided by Nick Sahinidis. Each instance has very balanced eigenvalues.}
  \end{table}

  \normalsize
  Remark
  \begin{itemize}
    \item In \redsf{rand-qcqp} instances, \(Q, A_i\) are low-rank indefinite, bb\_msc is effective.
    \item In \redsf{qplib-qcqp} instances each of which has very balanced eigenvalues. bb\_msc becomes less effective.
  \end{itemize}
\end{frame}

\begin{frame}[standout]{QCL}
  \begin{table}
    \begin{tabular}{llrrrrrl}
      \toprule
      {} & prob\_num   & solve\_time & best\_bound & best\_obj & node\_time & nodes  & method  \\
      \midrule
      0  & QPLIB\_2761 & 1479.828    & -0.0010     & -0.0010   & 0          & 2214.0 & grb     \\
      0  & QPLIB\_2761 & 1500.390    & 11.4674     & -0.2017   & 0.649      & 1142   & bb\_msc \\
      1  & QPLIB\_0018 & 43.730      & 6.3860      & 6.3860    & 0          & 2831.0 & grb     \\
      1  & QPLIB\_0018 & 1500.023    & 40.0342     & 5.8297    & 0.006      & 83458  & bb\_msc \\
      2  & QPLIB\_2712 & 377.647     & -0.0129     & -0.0129   & 0          & 859.0  & grb     \\
      2  & QPLIB\_2712 & 1500.005    & 6.4931      & -0.1455   & 0.073      & 6990   & bb\_msc \\
      \bottomrule
    \end{tabular}
  \end{table}

  Remark
  \begin{itemize}
    \item In \redsf{qplib-qcl} instances each of which has very balanced eigenvalues. bb\_msc becomes less effective.
  \end{itemize}
\end{frame}


\setlength\pdfpagewidth{16.00cm}%
\setlength\pdfpageheight{10.00cm}%

\begin{frame}[allowframebreaks]{Future Work}
  \begin{enumerate}
    \item Find low-rank graph applications. Max-cut, ...
    \item Start to migrate code to C++
    \item Find smarter rules to decide whether to start ADMM.
    \item Find better branching rules.
  \end{enumerate}
\end{frame}

\begin{frame}[allowframebreaks]{Future Work: Better Branching}

  Recall that we have a mutual \(V\) to convexify \({Q, A_i}\), if there is no \(\{A_i\}\), then \(V\) could be eigenvectors. This is not effective if it is not a low-rank matrix, i.e., there exists, \(v_1, ..., v_r\).

  Instead of branching on \(z_j = v_j^T x\)

  \begin{itemize}
    \item find \(p\) of our interest, moreover,
          \begin{align*}
            p^Tz \le p_0 \vee  p^Tz \ge p_0 \\
          \end{align*}
          \(pp^T\) is not diagonal so unable to create RLT cuts.
  \end{itemize}

\end{frame}


\begin{frame}[allowframebreaks]{\hypertarget{oldad}{Appendix: ADMM in the original \(x\)}}

  Notice,
  \begin{equation}
    \|x\|^2 = \max_{\|\xi\| \le \sqrt s} \xi^T x
  \end{equation}
  So we add slack variable \(s, t, \xi\) and bilinear constraint.
  \begin{align}
    \model{MSC} \quad \mathrm{Maximize: }\quad & y_0 ^T\lambda_0                     \\
    \mathrm{s.t.} \quad                        & (y,z,x) \in \Omega                  \\
                                               & y_i^Te \le t       & i=0, \cdots, m \\
    (\kappa) \quad                             & t= s               & i=0, \cdots, m \\
    (\mu)    \quad                             & \xi^Tx = t                          \\
                                               & \xi^T\xi \le s
  \end{align}
  If \(s, t, \xi, y, z, x\) is the solution, then \( y, z, x\) is the optimal solution for MSC.

  This allows the augmented Lagrangian function,

  \begin{align*}
    \mathscr L\left(x,y,z,\xi,s,\kappa,\mu\right) & = - y_0 ^T\lambda_0 + \kappa(t-s) + \mu(\xi^Tx - t) + \frac{\rho}{2}(t-s)^2 + \frac{\rho}{2}(\xi^Tx - s)^2
  \end{align*}

  The ADMM iteration,

  \begin{align*}
    (x,y,z,t)^{k+1} & = {\arg\min}_{(x,y,z)\in\Omega, t\ge 0} L\left(x,y,z,\xi^k,s^k,\kappa^k,\mu^k\right)       \\
    (s, \xi)^{k+1}  & = {\arg\min}_{(s, \xi)\in\mathscr{Q}} L\left((x,y,z,t)^{k+1},\xi,s, \kappa^k, \mu^k\right) \\
    \kappa^{k+1}    & = \kappa^k + \rho\left(t^{k+1}-s^{k+1}\right)                                              \\
    \mu^{k+1}       & = \mu^k + \rho\left( \langle\xi^{k+1}, x^{k+1}\rangle - s^{k+1}\right)
  \end{align*}
  where \(\mathscr{Q(\cdot)}\) forms a simple SOCP for \(s, \xi\),
  \begin{equation}
    \mathscr{Q}(x) =\left\{(s,\xi): \|\xi\|^2 \le s\right\}
  \end{equation}

\end{frame}



%%%%%%%%%%%%%%%
% appendix
%%%%%%%%%%%%%%%
% \begin{frame}[allowframebreaks]{Attempt for explicit disjunctions for inhomogeneous case}
%   \textbf{Alternatively}: branching on \(z - q^Tx\).

%   \textbf{Case I}: if \(z - q^Tx \le 0\), then,
%   \begin{align*}
%                                            & x^TRR^Tx - \underbrace{(q^Tx - z)}_{\Delta \ge 0} = (a^Tx)^2 \\
%     \text{Let: }\beta \ge 1 \Rightarrow \; & \red{\beta}\cdot \Delta\ge \|R^Tx\|^2                        \\
%     \Rightarrow \;                         & \red{(\beta - 1)}\cdot\Delta \ge (a^Tx)^2
%   \end{align*}

%   Can be rewritten as two rotated cones,
%   \begin{align*}
%     \beta + \Delta     & \ge \left\|\begin{array}{c} 2R^Tx \\ \beta - \Delta \end{array}\right\| \\
%     \beta - 1 + \Delta & \ge \left\|\begin{array}{c} 2a^Tx \\ \beta - 1 - \Delta \end{array}\right\| \\
%   \end{align*}
%   \framebreak

%   \textbf{Case II}: if \(z - q^Tx \ge 0\),

%   \begin{align*}
%     x^TRR^Tx +     & \underbrace{(z - q^Tx)}_{\Delta \ge 0} = (a^Tx)^2      \\
%                    & x^TRR^Tx + \red{\cancel{\Delta}} \le (a^Tx)^2          \\
%     \Rightarrow \; & \underbrace{a^Tx \ge \|R^Tx\|}_{\textsf{subcase II.1}}
%     \vee \underbrace{- a^Tx \ge \|R^Tx\|}_{\textsf{subcase II.2}}           \\
%   \end{align*}


%   This creates unbalanced subregions (same situation before.)

%   \framebreak

%   Try something similar to \textbf{Case I}

%   \begin{align*}
%     \beta \ge 1 \Rightarrow \; & \red{\beta}\cdot\Delta \ge (a^Tx)^2       \\
%                                & \red{(\beta - 1)}\cdot\Delta \ge (R^Tx)^2 \\
%                                & \red{\beta} \ge 1
%   \end{align*}

%   \(\Delta \doteq z - q^Tx\) is not bounded from above (we are maximizing \(z\))


%   \begin{align*}
%     \rho            & \ge (a^Tx)^2 \quad \dagger \\
%     \rho  -  \Delta & \ge \|R^Tx\|^2             \\
%   \end{align*}

%   \(\rho\) is not bounded from above \(\dagger\) will not be tight.
%   So the branch and bound is implemented on \(a^Tx \)
%   % Then we branch by last solution \(\rho^*\)
%   % \begin{equation*}
%   %   a^Tx \le \sqrt{\rho^*} \vee a^Tx \le \sqrt{\rho^*}
%   % \end{equation*}

% \end{frame}


\begin{frame}[allowframebreaks]{Bibliography}
  \bibliography{headers/qcqp}
  \bibliographystyle{apalike}
\end{frame}
\end{document}
